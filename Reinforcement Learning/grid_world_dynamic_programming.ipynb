{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9450723",
   "metadata": {},
   "source": [
    "## Dynamic programming\n",
    "\n",
    "Dynamic programming breaks a more complex problem down into smaller problems, making the overall problem simpler to solve. The solutions to these subproblems are saved and the larger problem is then easily solved when encountered. For this problem we assume we have an agent that want to learn the optimal policy for navigating the grid world where each action results in a reward of -1 and reaching a terminal state has a reward of 0. In other words, if the agent finds itself in any state, which action would it take to move towards the terminal state and maximize its rewards. The discount rate is set at 1, meaning future rewards are just as valuable as immediate rewards. To solve this problem we have the Bellman equation.\n",
    "\n",
    "Bellman equation\n",
    "\n",
    "Optimal policy $V^*$:\n",
    "- $V^*(s) = max_{a} Q^*(s,a)$\n",
    "\n",
    "Optimal state action pair (Q):\n",
    "- $Q^*(s,a) = \\sum_{s'} T(s,a,s') (R(s,a,s') + \\gamma V^*(s'))$\n",
    "\n",
    "Optimal policy $V^*$:\n",
    "- $V^*(s) = max_a \\sum_{s'} T(s,a,s') (R(s,a,s') + \\gamma V^*(s'))$\n",
    "\n",
    "Where:\n",
    "- $T$ is the transition probability for moving from $s$, taking action $a$, to $s'$\n",
    "    - Can also be expressed as $P(s'|s,a)$\n",
    "- $R$ is the reward for being in state $s$, taking action $a$, and ending up in $s'$\n",
    "- $\\gamma$ is the discount factor\n",
    "- $V^*(s')$ is the optimal policy from state $s'$, the state the agent moved to after taking action $a$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026a3ec",
   "metadata": {},
   "source": [
    "The discount factor affects the motivation of the agent to move towards the goal state, in this case the terminal state. Typically $0 < \\gamma \\leq 1$. If $\\gamma = 1$ the agent sees future rewards just as valuable as immediate rewards. Depending on the reward structure this could lead to an agent not taking the optimal policy and meandering around the environment with no incentive to reach the goal. If $\\gamma < 1$ future rewards are worth less the farther out they are, and again depending on the reward structure, the agent would seek to maximize rewards by moving to them quickly so as to not have a large discount factor reduce the rewards.\n",
    "\n",
    "The equations are actually very simple. Here's an example of walking through $Q^*(s,a)$\n",
    "\n",
    "This equation finds the value of the state, action pair. For example, the agent is in a state and wants to move north. The Q(s, north) tells us the value of moving north. All of these $Q(s,a)$ must be computed before we can find the $V^*(s)$. The value for terminal states is not updated.\n",
    "\n",
    "Say we have a random policy, so for each square in a grid world we can move north, south, east, or west with an equal probability, so 25% chance. This becomes the transition probability. If the agent moves off the grid it just stays in place. To clarify, if the agent chooses the action to move north, there is a 25% chance of moving north, as well as a 25% chance of moving in one of the other three options (S,E,W).\n",
    "\n",
    "For this example lets picture a very simple 3x3 grid with the agent starting in the center at (2,2). The rewards are 0 unless the agent is in a terminal state (3,3) where $R = 1$, and the discount factor is $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa506e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "gw_ = np.zeros((3,3))\n",
    "gw = np.zeros((3,3), dtype='<U5')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        gw[i,j] = f\"{i+1},{j+1}\"\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82185493",
   "metadata": {},
   "source": [
    "$Q^*(s,a) = \\sum_{s'} T(s,a,s') (R(s,a,s') + \\gamma V^*(s'))$\n",
    "\n",
    "North:\n",
    "- $Q^*(s (2,2) , north) = 0.25 *  (-1 + 1* V^*(1,2))$\n",
    "- $Q^*(s (2,2) , north) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "South:\n",
    "- $Q^*(s (2,2) , south) = 0.25 *  (-1 + 1* V^*(3,2))$\n",
    "- $Q^*(s (2,2) , south) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "East:\n",
    "- $Q^*(s (2,2) , east) = 0.25 *  (-1 + 1* V^*(2,3))$\n",
    "- $Q^*(s (2,2) , east) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "West:\n",
    "- $Q^*(s (2,2) , west) = 0.25 *  (-1 + 1* V^*(2,1))$\n",
    "- $Q^*(s (2,2) , west) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "Now we sum these values and return an overall value of $Q^*(s, north) = -1$\n",
    "\n",
    "We need to do this again for $Q^*(s, south)$, $Q^*(s, east)$, and $Q^*(s, west)$. Because this is the first iteration and every state has been initialized to a value of 0 we get 0 for each state action pair. We then calculate $V^*(2,2)$ using the equation below.\n",
    "\n",
    "$V^*(2,2) = max_a(Q(22, N),Q(22, S),Q(22, E),Q(22, W))$\n",
    "\n",
    "$V^*(2,2) = max_a(-1, -1, -1, -1)$\n",
    "\n",
    "$V^*(2,2) = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a9fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[ 0.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "gw_[1,1] = -1\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e086dc",
   "metadata": {},
   "source": [
    "Now say we moved south to state 3,2. We need to perform the above steps again, except that this time there is a reward of 1 in state 3,3, and we also have a value of -1 in state 2,2.\n",
    "\n",
    "North:\n",
    "- $Q^*(s (3,2) , north) = 0.25 *  (-1 + 1* V^*(2,2))$\n",
    "- $Q^*(s (3,2) , north) = 0.25 *  (-1 + 1* -1) = -0.5$\n",
    "\n",
    "South:\n",
    "- The agent stays in place if it tries to move off the grid\n",
    "- $Q^*(s (3,2) , south) = 0.25 *  (-1 + 1* V^*(3,2))$\n",
    "- $Q^*(s (3,2) , south) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "East:\n",
    "- State (3,3) is a terminal state and has a reward of 1\n",
    "- The value of a  terminal state is not updated\n",
    "- $Q^*(s (3,2) , east) = 0.25 *  (-1 + 1* V^*(3,3))$\n",
    "- $Q^*(s (3,2) , east) = 0.25 *  (-1 + 1* 1) = 0$\n",
    "\n",
    "West:\n",
    "- $Q^*(s (3,2) , west) = 0.25 *  (-1 + 1* V^*(3,1))$\n",
    "- $Q^*(s (3,2) , west) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "$Q^*((3,2), north) = -0.5+-0.25+0+-0.25 = -1$\n",
    "\n",
    "We do this for each of the other actions (S,E,W) to get the following.\n",
    "\n",
    "$Q^*((3,2), south) = -0.5+-0.25+0+-0.25 = -1$\n",
    "\n",
    "$Q^*((3,2), east) = -0.5+-0.25+0+-0.25 = -1$\n",
    "\n",
    "$Q^*((3,2), west) = -0.5+-0.25+0+-0.25 = -1$\n",
    "\n",
    "\n",
    "Now we take the max of these values to update $V(3,2)$.\n",
    "\n",
    "$V^*(3,2) = max_a(-1, -1, -1, -1) = -1$\n",
    "\n",
    "This may not seem like much is happening, but what if we continue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a473833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[ 0.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "gw_[2,1] = -1\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2f514",
   "metadata": {},
   "source": [
    "Now say we moved south to state 3,3. This is a terminal state where the reward is 1. We cannot select an action from a terminal state because the episode has ended. We could update the value of state (3,3) as follows, but typically it is just set to the reward value without bothering to calculate all of these equations as the result it simply the reward value.\n",
    "\n",
    "North:\n",
    "- $Q^*(s (3,2) , north) = 1.0 *  (1)$\n",
    "- $Q^*(s (3,2) , south) = 1.0 *  (1)$\n",
    "- $Q^*(s (3,2) , east) = 1.0 *  (1)$\n",
    "- $Q^*(s (3,2) , west) = 1.0 *  (1)$\n",
    "\n",
    "Now we take the max of these values to update $V(3,3)$.\n",
    "\n",
    "$V^*(3,2) = max_a(1, 1, 1, 1) = 1$\n",
    "\n",
    "When an agent finishes an episode by entering a terminal state the agent is typically reset. Here we'll reset to S(2,2) and recalculate the values we previously calculated. Below is the updated grid world with the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03739cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[ 0.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0. -1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "gw_[2,2] = 1\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19b140",
   "metadata": {},
   "source": [
    "Starting over in state (2,2)\n",
    "\n",
    "North:\n",
    "- $Q^*(s (2,2) , north) = 0.25 *  (-1 + 1* V^*(1,2))$\n",
    "- $Q^*(s (2,2) , north) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "South:\n",
    "- $Q^*(s (2,2) , south) = 0.25 *  (-1 + 1* V^*(3,2))$\n",
    "- $Q^*(s (2,2) , south) = 0.25 *  (-1 + 1* -1) = -0.5$\n",
    "\n",
    "East:\n",
    "- $Q^*(s (2,2) , east) = 0.25 *  (-1 + 1* V^*(2,3))$\n",
    "- $Q^*(s (2,2) , east) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "West:\n",
    "- $Q^*(s (2,2) , west) = 0.25 *  (-1 + 1* V^*(2,1))$\n",
    "- $Q^*(s (2,2) , west) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "Now we sum these values and return an overall value of $Q^*(s, north) = -1.25$\n",
    "\n",
    "We need to do this again for $Q^*(s, south)$, $Q^*(s, east)$, and $Q^*(s, west)$. Because each action has a 25% of being taken for each possible direction we end up with the same values for each action again. We then calculate $V^*(2,2)$ using the equation below.\n",
    "\n",
    "$V^*(2,2) = max_a(Q(22, N),Q(22, S),Q(22, E),Q(22, W))$\n",
    "\n",
    "$V^*(2,2) = max_a(-1.25, -1.25, -1.25, -1.25)$\n",
    "\n",
    "$V^*(2,2) = -1.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b49ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[ 0.    0.    0.  ]\n",
      " [ 0.   -1.25  0.  ]\n",
      " [ 0.   -1.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "gw_[1,1] = -1.25\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ce0eb",
   "metadata": {},
   "source": [
    "Now we do state (3,2)\n",
    "\n",
    "Now say we moved south to state 3,2. We need to perform the above steps again, except that this time there is a reward of 1 in state 3,3, and we also have a value of -1 in state 2,2.\n",
    "\n",
    "North:\n",
    "- $Q^*(s (3,2) , north) = 0.25 *  (-1 + 1* V^*(2,2))$\n",
    "- $Q^*(s (3,2) , north) = 0.25 *  (-1 + 1* -1.25) = -0.3125$\n",
    "\n",
    "South:\n",
    "- The agent stays in place if it tries to move off the grid\n",
    "- $Q^*(s (3,2) , south) = 0.25 *  (-1 + 1* V^*(3,2))$\n",
    "- $Q^*(s (3,2) , south) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "East:\n",
    "- State (3,3) is a terminal state and has a reward of 1\n",
    "- The value of a  terminal state is not updated\n",
    "- $Q^*(s (3,2) , east) = 0.25 *  (-1 + 1* V^*(3,3))$\n",
    "- $Q^*(s (3,2) , east) = 0.25 *  (-1 + 1* 1) = 0$\n",
    "\n",
    "West:\n",
    "- $Q^*(s (3,2) , west) = 0.25 *  (-1 + 1* V^*(3,1))$\n",
    "- $Q^*(s (3,2) , west) = 0.25 *  (-1 + 1* 0) = -0.25$\n",
    "\n",
    "$Q^*((3,2), north) = -0.3125+-0.25+0+-0.25 = -0.8125$\n",
    "\n",
    "Again, We do this for each of the other actions (S,E,W) and currently they have the same results, giving the following.\n",
    "\n",
    "$Q^*((3,2), south) = -0.3125+-0.25+0+-0.25 = -0.8125$\n",
    "\n",
    "$Q^*((3,2), east) = -0.3125+-0.25+0+-0.25 = -0.8125$\n",
    "\n",
    "$Q^*((3,2), west) = -0.3125+-0.25+0+-0.25 = -0.8125$\n",
    "\n",
    "\n",
    "Now we take the max of these values to update $V(3,2)$.\n",
    "\n",
    "$V^*(3,2) = max_a(-0.8125,-0.8125,-0.8125,-0.8125) = -0.8125$\n",
    "\n",
    "Below we can see the updated values. You can see that the values are beginning to converge in a way to moves the agent towards the terminal state. In the interest of not doing a ton of calculations we skipped the other states, but notice the ones we did the calculations for. If the agent want the maximum value as a reward, it could use the values of states to move from (2,2) to (2,3) and then to (3,3) (again, ignoring the states we didn't calculate yet). With enough iterations the values will converge to stable values that can be used in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489ccdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      " [['1,1' '1,2' '1,3']\n",
      " ['2,1' '2,2' '2,3']\n",
      " ['3,1' '3,2' '3,3']]\n",
      "State Values\n",
      " [[ 0.      0.      0.    ]\n",
      " [ 0.     -1.25    0.    ]\n",
      " [ 0.     -0.8125  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "gw_[2,1] = -0.8125\n",
    "print(\"States\\n\",gw)\n",
    "print(\"State Values\\n\",gw_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c11e68",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Mini grid world example for policy evaluation using dynamic programming from the book, Reinforcement Learning (Sutton chapter 4).\n",
    "\n",
    "Solves policy evaluation and returns policy values and matrix with \"arrows\" showing which action to take next.\n",
    "\n",
    "Gridworld is 4x4 with first and last cells being terminal states. The challenge is to find convergence of values for states through policy iteration. You can change the size of the grid world and whether there are 1 or 2 terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bd45a",
   "metadata": {},
   "source": [
    "There are two possible terminal states, either the bottom right state, or the bottom right state and the top left state. Change this by using the kwargs = 'first_and_last' or omit to keep just the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4233b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldSolver():\n",
    "    def __init__(self, rewards = -1, world_size = (4,4), gamma = 1, theta = 1e-5, **kwargs):\n",
    "        self.r = rewards\n",
    "        self.world = np.zeros((world_size[0], world_size[1]))\n",
    "        self.g = gamma\n",
    "        self.theta = theta\n",
    "        self.terminals = kwargs.get('terminals', 'last')\n",
    "        self.iterations = kwargs.get('iterations', 0)\n",
    "\n",
    "\n",
    "    def v_pi(self, directions):\n",
    "        \"\"\"\n",
    "        returns value for state\n",
    "        params:\n",
    "            directions:\n",
    "        returns:\n",
    "            total: sum of one_dir() which is return of all possible actions in that state\n",
    "            (return from up, down, r, l x the prob of each (1/4) summed to represent the new v(s) for that space)\n",
    "        \"\"\"\n",
    "\n",
    "        total = 0\n",
    "        d = ['up','down','left','right']\n",
    "        for i, direction in enumerate(directions):\n",
    "            single_dir = (1/len(d)) * (self.r + self.g * self.world[direction])\n",
    "            total += single_dir\n",
    "        return total\n",
    "\n",
    "\n",
    "    def calc_mini_gridworld(self):\n",
    "        \"\"\"calculates values of states until convergence\n",
    "        params: None\n",
    "        returns:\n",
    "            self.world: array of updated values representing values of each state in grid\n",
    "        \"\"\"\n",
    "        \n",
    "        delta = np.inf\n",
    "        \n",
    "        if self.iterations:\n",
    "            for i in range(self.iterations):\n",
    "                self.iterate()\n",
    "        else:\n",
    "            i = 0\n",
    "            while delta > self.theta:\n",
    "                delta = self.iterate()\n",
    "                i += 1\n",
    "                if delta < self.theta:\n",
    "                    break\n",
    "\n",
    "        print(f\"Iterations completed : {i+1}\")\n",
    "\n",
    "        return self.world\n",
    "    \n",
    "\n",
    "    def iterate(self):\n",
    "        \"\"\"iterates over states calculating and updating values with each iteration\n",
    "        params: None\n",
    "        returns:\n",
    "            if not using fixed number of iterations:\n",
    "                delta: change in values to compare against previous iteration to check for convergence\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp_world = copy.deepcopy(self.world)\n",
    "\n",
    "        for i in range(self.world.shape[0]):\n",
    "            for j in range(self.world.shape[1]):\n",
    "\n",
    "                if(i == self.world.shape[0]-1 and j == self.world.shape[1]-1):\n",
    "                    pass\n",
    "                elif (i == 0 and j == 0) and self.terminals == 'first_and_last':\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    if i-1 < 0:\n",
    "                        up = i\n",
    "                    else:\n",
    "                        up = i-1\n",
    "                    if i+1 > self.world.shape[0] - 1:\n",
    "                        down = i\n",
    "                    else:\n",
    "                        down = i+1\n",
    "                    if j-1 < 0:\n",
    "                        left = j\n",
    "                    else:\n",
    "                        left = j-1\n",
    "                    if j+1 > self.world.shape[1] - 1:\n",
    "                        right = j\n",
    "                    else:\n",
    "                        right = j+1\n",
    "\n",
    "                    directions = [(up,j),(down,j), (i,left), (i,right)]\n",
    "                    self.world[i,j] = self.v_pi(directions)\n",
    "\n",
    "        if not self.iterations:\n",
    "            w_flat = np.squeeze(self.world.reshape(1,-1))[1:]\n",
    "            tmp_flat = np.squeeze(tmp_world.reshape(1,-1))[1:]\n",
    "            delta = max(abs(w_flat - tmp_flat))\n",
    "            return delta\n",
    "\n",
    "\n",
    "    def add_arrows(self, grid_world):\n",
    "        \"\"\"translates numbers to actions\"\"\"\n",
    "\n",
    "        action_world = np.zeros(grid_world.shape, dtype='<U2')\n",
    "        for i in range(grid_world.shape[0]):\n",
    "            for j in range(grid_world.shape[1]):\n",
    "                if(i == self.world.shape[0]-1 and j == self.world.shape[1]-1):\n",
    "                    action_world[i,j] = 'G'\n",
    "                elif (i == 0 and j == 0) and self.terminals == 'first_and_last':\n",
    "                    action_world[i,j] = 'G'\n",
    "                else:\n",
    "                    if i-1 < 0:\n",
    "                        up = None\n",
    "                    else:\n",
    "                        up = i-1\n",
    "                    if i+1 > grid_world.shape[0] - 1:\n",
    "                        down = None\n",
    "                    else:\n",
    "                        down = i+1\n",
    "                    if j-1 < 0:\n",
    "                        left = None\n",
    "                    else:\n",
    "                        left = j-1\n",
    "                    if j+1 > grid_world.shape[1] - 1:\n",
    "                        right = None\n",
    "                    else:\n",
    "                        right = j+1\n",
    "                    \n",
    "                    directions = [(up,j),(down,j), (i,left), (i,right)]\n",
    "                    d = ['up','down','left','right']\n",
    "                    actions = ''\n",
    "                    for k, direction in enumerate(directions):\n",
    "                        try:\n",
    "                            if grid_world[direction[0],direction[1]] > grid_world[i,j]:\n",
    "                                if k == 0:\n",
    "                                    actions += '^'\n",
    "                                elif k == 1:\n",
    "                                    actions += 'v'\n",
    "                                elif k == 2:\n",
    "                                    actions += '<' \n",
    "                                elif k == 3:\n",
    "                                    actions += '>'\n",
    "                                if len(actions) == 1:\n",
    "                                    actions += ' '\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    action_world[i,j] = actions\n",
    "        return action_world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab4bf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwargs):\n",
    "    terminals = kwargs.get('terminals','last')\n",
    "    iterations = kwargs.get('iterations', 0)\n",
    "    mini_grid_world = GridWorldSolver(terminals = terminals, \n",
    "                                      iterations = iterations)\n",
    "    grid_world = mini_grid_world.calc_mini_gridworld()\n",
    "    print(grid_world.round(1))\n",
    "    diagram = mini_grid_world.add_arrows(grid_world)\n",
    "    print(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e553e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations completed : 383\n",
      "[[-59.4 -57.4 -54.3 -51.7]\n",
      " [-57.4 -54.6 -49.7 -45.1]\n",
      " [-54.3 -49.7 -40.9 -30. ]\n",
      " [-51.7 -45.1 -30.    0. ]]\n",
      "[['v ' 'v ' 'v ' 'v ']\n",
      " ['v ' 'v ' 'v ' 'v ']\n",
      " ['v ' 'v ' 'v ' 'v ']\n",
      " ['> ' '> ' '> ' 'G']]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71a785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations completed : 100\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "[['G' '< ' '< ' 'v ']\n",
      " ['^ ' '^ ' '^ ' 'v ']\n",
      " ['^ ' '^ ' 'v ' 'v ']\n",
      " ['^ ' '^ ' '> ' 'G']]\n"
     ]
    }
   ],
   "source": [
    "main(terminals = 'first_and_last', iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f5497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
